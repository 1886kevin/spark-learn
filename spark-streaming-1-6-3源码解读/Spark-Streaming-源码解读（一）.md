##  NetworkWordCount例子
废话不多说，我就直接从spark源码中的NetworkWordCount中的这里开始走读源码。

```
/** Counts words in UTF8 encoded, '\n' delimited text received from the network every second. 
  * Usage: NetworkWordCount <hostname> <port> * <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data. 
  * To run this on your local machine, you need to first run a Netcat server 
  *    `$ nc -lk 9999` * and then run the example 
  *    `$ bin/run-example org.apache.spark.examples.streaming.NetworkWordCount localhost 9999` 
*/

object NetworkWordCount {  
    def main(args: Array[String]) {    
       if (args.length < 2) {      
            System.err.println("Usage: NetworkWordCount <hostname> <port>")      
            System.exit(1)    
      }    
      StreamingExamples.setStreamingLogLevels()    
      // Create the context with a 1 second batch size    
      val sparkConf = new SparkConf().setAppName("NetworkWordCount")    
      val ssc = new StreamingContext(sparkConf, Seconds(1))    
      // Create a socket stream on target ip:port and count the    
      // words in input stream of \n delimited text (eg. generated by 'nc')
      // Note that no duplication in storage level only for running locally
      // Replication necessary in distributed scenario for fault tolerance.
      val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER)    
      val words = lines.flatMap(_.split(" "))    
      val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
      wordCounts.print()    
      ssc.start()    
      ssc.awaitTermination()  
   }
}
// scalastyle:on println

```
这段代码实现的功能就是每隔1秒处理一次从Netcat server获得的数据，这里需要注意的是：

**这个程序中是每隔1秒中处理一次接收的数据，而数据的接收是由RateLimiter来控制的，关于这个类后面会给大家详细介绍**

##  源码走读
下面我们一行一行的来分析代码：
### 第一步 Spark Streaming应用的初始化
创建Spark Streaming应用程序的配置信息
```
 // Create the context with a 1 second batch size    
val sparkConf = new SparkConf().setAppName("NetworkWordCount")    
val ssc = new StreamingContext(sparkConf, Seconds(1))    
```
spark应用程序中有好多配置信息可以配置，而程序运行时，其配置信息就被加载到这个类中了，具体是通过conf文件夹下的文件来配置的，大家比较熟悉的spark-defaults.conf，spark-env.sh，hive-site.xml，
metrics.properties等，当然这里也可以通过SparkConf这个类中的set方法指定，如：
```
sparkConf.set("spark.app.name", name)   
```
创建Spark Streaming应用程序上下文：
```
val ssc = new StreamingContext(sparkConf, Seconds(1))    
```
其中第二个参数指定Spark Streaming应用的批处理时间间隔，即每隔多长时间处理一次数据。

下面直接看这个StreamingContext初始化了一些什么内容：
首先看我们这个例子使用的StreamingContext的构造函数：这里会创建一个SparkContext，SparkContext是spark cores程序运行的上下文环境，其实spark程序的运行都是建立在spark cores的基础上的。关于spark cores的运行，后续在谈。

```
def this(conf: SparkConf, batchDuration: Duration) = {  
this(StreamingContext.createNewSparkContext(conf), null, batchDuration)
}

private[streaming] def createNewSparkContext(conf: SparkConf): SparkContext = {
  new SparkContext(conf)
}
```
然后在StreamingContext中会初始化一些必要的变量和执行一些初始化的操作：

```
//检查SparkContext和checkpoint目录是否为null，程序运行时可以不checkpoint，
//但一定需要SparkContext，因为SparkStream只是运行在Spark Core的基础上的一个框架，
//其执行是依赖Spark Core的。
if (sc_ == null && cp_ == null) {  
    throw new Exception("Spark Streaming cannot be initialized with " +  "both SparkContext and checkpoint as null")
}
//判断当前的Spark Streaming程序是否checkpoint
private[streaming] val isCheckpointPresent = (cp_ != null)
//sc的赋值，如没有配置checkpoint，则直接返回前面创建的
//SparkContext，若配置了checkpoint，则需从当前应用中读取properties信息，
//生成新的sparconf，并根据这个sparkconf创建一个新的SparkContext
private[streaming] val sc: SparkContext = {  
  if (sc_ != null) { 
   sc_ 
 } else if (isCheckpointPresent) {    
  SparkContext.getOrCreate(cp_.createSparkConf())  
} else {    
  throw new SparkException("Cannot create StreamingContext without a SparkContext")  
  }
}
//若当前程序运行在本地，则必须为其至少分配两个core，因为其中肯定需要有一个core运行reciever线程
if (sc.conf.get("spark.master") == "local" || sc.conf.get("spark.master") == "local[1]") {  
  logWarning("spark.master should be set as local[n], n > 1 in local mode if you have receivers" +    " to get data, otherwise Spark jobs will not get resources to process the received data.")
}
//配置SparkConf
private[streaming] val conf = sc.conf
//spark 运行时环境，即Spark core中的SparkEnv
private[streaming] val env = sc.env
//DStreamGraph功能类似于Spark Core的DAGScheduler，负责上层pipeline的生成
private[streaming] val graph: DStreamGraph = {  
  if (isCheckpointPresent) {    
    cp_.graph.setContext(this)    
    //从checkpoint目录中恢复数据，其中主要是恢复outputStreams的处理信息
    // 即读取checkpoint的信息，并为其及其dependencies生成ReliableCheckpointRDD
    cp_.graph.restoreCheckpointData()    
    cp_.graph  
  } else {    
    require(batchDur_ != null, "Batch duration for StreamingContext cannot be null")    
    val newGraph = new DStreamGraph()   
    newGraph.setBatchDuration(batchDur_)   
    newGraph  
  }
}
//输入流的id，一个Spark Streaming程序中可能有多个输入流
private val nextInputStreamId = new AtomicInteger(0)
//checkpointDir即checkpoint的目录，这里需要说明的是如果该程序运行在集群上则必须是一个HDFS的路径
private[streaming] var checkpointDir: String = {  
  if (isCheckpointPresent) {    
    sc.setCheckpointDir(cp_.checkpointDir)    
    cp_.checkpointDir  
  } else {    
    null  
  }
}
//执行checkpoint的时间间隔
private[streaming] val checkpointDuration: Duration = {  
  if (isCheckpointPresent) 
    cp_.checkpointDuration 
  else 
    graph.batchDuration
}
//这个变量是最重要的，它负责生成job。
private[streaming] val scheduler = new JobScheduler(this)
//主要构建一个线程的同步锁对象，这个对象主要在停止部分会被调用 
//这里的这个waiter对象其实就是初始化部分的ContextWaiter,其主要作用：
//等待执行停止。执行期间发生的任何异常将被扔在这个线程。
private[streaming] val waiter = new ContextWaiter
//这个变量功能就是监听Spark Streaming程序的运行，其内部的变量有
//private val waitingBatchUIData = new HashMap[Time, BatchUIData]
//private val runningBatchUIData = new HashMap[Time, BatchUIData]
//private val completedBatchUIData = new Queue[BatchUIData]
//private val batchUIDataLimit = ssc.conf.getInt("spark.streaming.ui.retainedBatches", 1000)
//private var totalCompletedBatches = 0L
//private var totalReceivedRecords = 0L
//private var totalProcessedRecords = 0L
//private val receiverInfos = new HashMap[Int, ReceiverInfo]
//当然这个类是继承SparkListener，所以它也是Spark系统消息系统的一部分
//现在基本上所有的分布式系统都是通过消息驱动的方式来进行并发执行的
private[streaming] val progressListener = new StreamingJobProgressListener(this)
//这个变量只要是为该程序生成ui界面的
private[streaming] val uiTab: Option[StreamingTab] =  if (conf.getBoolean("spark.ui.enabled", true)) {    
  Some(new StreamingTab(this))  
  } else {    
    None  
  }
/* Initializing a streamingSource to register metrics */
 //这个变量主要是为我们的监控系统Ganglia来对该程序进行监控的，它属于spark测量系统的一部分，可以获取当前程序运行基本信息，
 如receivers，totalCompletedBatches，totalReceivedRecords，runningBatches，lastCompletedBatch_submissionTime等信息。
  private val streamingSource = new StreamingSource(this)
 //当前Spark Streaming应用的状态，应用的状态包括INITIALIZED,ACTIVE,STOPPED三种。
  private var state: StreamingContextState = INITIALIZED
 //用户代码中调用spark接口的堆栈信息方法中
 //其具体功能实现在Utils.scala的getCallSite
 //功能描述：获取当前SparkContext的当前调用堆栈，将栈里最靠近栈底的属于spark或者Scala核心的类压入callStack的栈顶，
 //并将此类的方法存入lastSparkMethod；将栈里最靠近栈顶的用户类放入callStack，将此类的行号存入firstUserLine，
 //类名存入firstUserFile，最终返回的样例类CallSite存储了最短栈和长度默认为20的最长栈的样例类。
 //在JavaWordCount例子中，获得的数据如下：
 //最短栈：JavaSparkContext at JavaWordCount.java:44；
 //最长栈：org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
 //org.apache.spark.examples.JavaWordCount.main(JavaWordCount.java:44)。
 //这个变量其实主要是给开发者看的，对于具体程序的运行没有必然的影响。当然它的信息也会反应在Spark ui的界面上的
  private val startSite = new AtomicReference[CallSite](null)
 //当应用程序停止时会调用该对象，不管是正常退出或异常退出，stopOnShutdown()方法都会被回调，
 //然后调用stop方法。stopGracefully 可以通过配置项spark.streaming.stopGracefullyOnShutdown配置，生产环境需要配置为true.
 //关于更具体的实现，请大家看下这篇博客http://www.jianshu.com/p/18cd94b5c647
  private var shutdownHookRef: AnyRef =conf.getOption("spark.streaming.checkpoint.directory").foreach(checkpoint)
```

### 第二步 Spark Streaming应用的业务逻辑（Lazy级别）

之后的代码就是这个Spark Streaming程序的业务逻辑部分了。
构建InputDStream
```
  val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER)
```
这一行代码，我们先看一下SparkStreamingContext中该方法的实现：

```
   def socketTextStream(
      hostname: String,
      port: Int,
      storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2
    ): ReceiverInputDStream[String] = withNamedScope("socket text stream") {
    socketStream[String](hostname, port, SocketReceiver.bytesToLines, storageLevel)
  }
该方法又调用了下面这个方法
 def socketStream[T: ClassTag](
      hostname: String,
      port: Int,
      converter: (InputStream) => Iterator[T],
      storageLevel: StorageLevel
    ): ReceiverInputDStream[T] = {
    new SocketInputDStream[T](this, hostname, port, converter, storageLevel)
  }

其中SocketInputDStream的实现为：

class SocketInputDStream[T: ClassTag](
    ssc_ : StreamingContext,
    host: String,
    port: Int,
    bytesToObjects: InputStream => Iterator[T],
    storageLevel: StorageLevel
  ) extends ReceiverInputDStream[T](ssc_) {

  def getReceiver(): Receiver[T] = {
    new SocketReceiver(host, port, bytesToObjects, storageLevel)
  }
}
这里会提供一个创建receiver的方法。其中SocketInputDStream继承自ReceiverInputDStream，而ReceiverInputDStream又继承自

abstract class InputDStream[T: ClassTag] (ssc_ : StreamingContext)
  extends DStream[T](ssc_) {

  private[streaming] var lastValidTime: Time = null
  ssc.graph.addInputStream(this)
这其中有一个很重要的操作就是把这个SocketInputDStream添加到ssc中的DStreamGraph中，而在Spark Streaming 中的DStreamGraph功能类似于Spark Core中的DAGScheduler，DStreamGraph是Spark Streaming的程序中job的high level 级别的实现。
```
所以这一段代码具体实现了三个重要的功能：
（1）提供一个InputDStream，用来处理receiver接收到的数据
（2）提供一个获取receiver的方法
（3）将该DStream添加到DStreamGragh中

“真正”的业务逻辑，接下来的代码如下：
```
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
    wordCounts.print()
```
这里每进行一个操作就会生成一个新的DSream，最终这段代码会生成一个DStream链，在这里你可以简单的认为最后的print操作是action级别的，其余的都是transformation级别。

注意：DStream的操作和RDD是一样的（当然DStream没有RDD实现的算法多），其唯一的区别就是RDD是处理固定的数据（不变的数据），而DSream处理的是固定时间内生成的数据（变化的数据）。阅读过源码就会发现其实Spark Streaming程序和Spark Core均是处理的RDD，所以DStream可以直接理解成RDD的模版，它是每次任务调用时才真正的实例化RDD，然后最终的操作还是对RDD的操作。

下面看一下print这个方法的具体实现：

```
def print(num: Int): Unit = ssc.withScope {
    def foreachFunc: (RDD[T], Time) => Unit = {
      (rdd: RDD[T], time: Time) => {
        val firstNum = rdd.take(num + 1)
        // scalastyle:off println
        println("-------------------------------------------")
        println("Time: " + time)
        println("-------------------------------------------")
        firstNum.take(num).foreach(println)
        if (firstNum.length > num) println("...")
        println()
        // scalastyle:on println
      }
    }
    foreachRDD(context.sparkContext.clean(foreachFunc), displayInnerRDDOps = false)
  }

其中它又跳用了foreachRDD这个方法：

  private def foreachRDD(
      foreachFunc: (RDD[T], Time) => Unit,
      displayInnerRDDOps: Boolean): Unit = {
    new ForEachDStream(this,
      context.sparkContext.clean(foreachFunc, false), displayInnerRDDOps).register()
  }
然后这个方法中也生成了一个DStream，最后调用了register方法
而这个方法的实现：
private[streaming] def register(): DStream[T] = {  
  ssc.graph.addOutputStream(this) 
  this
}

这里它会把这个DStream当作OutputStream添加到DStreamGragh中，然后在程序启动后根据DStreamGragh来生成业务的具体实现。
其过程就是先反向生成业务计算的pipeline，然后在真正计算的时候在根据pipeline正向进行。（其方式和RDD一摸一样）
```
如果代码执行到这，整个Spark Streaming应用根本不会执行任何操作，因为上面的那些操作都是lazy级别的，只有在程序启动后才会去真正执行。
### 第三步 Spark Streaming应用的启动

程序的启动：
```
ssc.start()
ssc.awaitTermination()
```
然后执行start方法启动执行，关于这个执行过程，它还有很多的步骤，这个会在下篇博客中具体谈。
之后就是这个ssc.awaitTermination()方法，这个方法我就是觉得挺有用的，然后就说一下：

```
 def awaitTermination() {
    waiter.waitForStopOrError()
  }

而其中这个waiter就是在SparkStreamingContext初始化的时候来赋值
private[streaming] val waiter = new ContextWaiter 

这里这个方法的实现是：

 def awaitTermination() {
    waiter.waitForStopOrError()
  }

  def waitForStopOrError(timeout: Long = -1): Boolean = {
    lock.lock()
    try {
      if (timeout < 0) {
        while (!stopped && error == null) {
          condition.await()
        }
      } else {
        var nanos = TimeUnit.MILLISECONDS.toNanos(timeout)
        while (!stopped && error == null && nanos > 0) {
          nanos = condition.awaitNanos(nanos)
        }
      }
      // If already had error, then throw it
      if (error != null) throw error
      // already stopped or timeout
      stopped
    } finally {
      lock.unlock()
    }
  }

  这就是一个简单线程锁，用来等待Spark Streaming程序正常或者异常退出。这里它有两个方法，用来解这个锁：
  
  def notifyError(e: Throwable): Unit = {
    lock.lock()
    try {
      error = e
      condition.signalAll()
    } finally {
      lock.unlock()
    }
  }

  def notifyStop(): Unit = {
    lock.lock()
    try {
      stopped = true
      condition.signalAll()
    } finally {
      lock.unlock()
    }
  }
```

   这里只是简单介绍一下这个功能，如果以后我们开发Spark程序时，如果有需要的话可以直接使用，当然不是spark程序你也可以使用，这个对我们整个程序的理解没有太大关系。只是觉得这个方法以后可能会对我们写程序有帮助。
  
好了，就到这里，现在一个程序的基本过程都讲的很清楚了，下面直接就讲它这个start方法是怎么来真正驱动spark Streaming应用进行的了。
