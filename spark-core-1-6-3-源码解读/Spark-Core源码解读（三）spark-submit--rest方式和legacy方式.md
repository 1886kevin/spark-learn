å¯¹äºspark-submitçš„æäº¤ï¼Œspark1.3.0ä¹‹åæå‡ºäº†restçš„æ–¹å¼æ¥æäº¤åº”ç”¨ï¼ŒåŸæ¥çš„å°±æ˜¯legacyæ–¹å¼ï¼Œä¸æ˜¯åŸæ¥å°±æ˜¯legacyï¼Œæ˜¯å› ä¸ºlegacyçš„è¿™ä¸ªå•è¯å°±æ˜¯åŸæ¥çš„æ„æ€ã€‚
 
### spark-submit  restæ–¹å¼
å…³äºspark-submitæäº¤appï¼Œè¿™é‡Œå°±ç›´æ¥ä»å®˜æ–¹çš„ä¸€ä¸ªå°ä¾‹å­è°ˆèµ·ï¼ˆæ³¨æ„ä¸‹é¢6066æ˜¯å› ä¸ºæˆ‘ä»¬åœ¨masterä¸Šå¯åŠ¨çš„é‚£ä¸ªrestserverçš„ç«¯å£å·å°±æ˜¯6066ï¼‰ï¼š
```
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://207.184.161.138:6066 \
  /path/to/examples.jar   
```
å®ƒè¿™é‡Œä¼šä½¿ç”¨binç›®å½•ä¸‹çš„spark-submit.shï¼Œä¸‹é¢ç›´æ¥çœ‹è¿™ä¸ªshellè„šæœ¬ï¼š

```
if [ -z "${SPARK_HOME}" ]; then
  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"
fi

# disable randomized hash for string in Python 3.3+
export PYTHONHASHSEED=0

1. è¿™é‡Œæœ€é‡è¦çš„æ˜¯è¿™å¥ä»£ç ï¼Œå®ƒæŒ‡ç¤ºè¦å¯åŠ¨ org.apache.spark.deploy.SparkSubmitè¿™ä¸ªç±»
exec "${SPARK_HOME}"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"
  
```
ç„¶åå°±ç›´æ¥çœ‹è¿™ä¸ªç±»ä¸­çš„mainæ–¹æ³•ï¼š
```
 ......
 
 def main(args: Array[String]): Unit = {
    1. æ„é€ å‚æ•°çš„ç±»ï¼Œç€é‡çœ‹ä¸€ä¸‹è¿™ä¸ªç±»
    val appArgs = new SparkSubmitArguments(args)
    if (appArgs.verbose) {
      // scalastyle:off println
      printStream.println(appArgs)
      // scalastyle:on println
    }
    appArgs.action match {
      2. æ ¹æ®ä¸Šé¢æä¾›çš„å‚æ•°ï¼ŒåŒ¹é…æ‰§è¡Œç›¸åº”çš„æ“ä½œ
      case SparkSubmitAction.SUBMIT => submit(appArgs)
      case SparkSubmitAction.KILL => kill(appArgs)
      case SparkSubmitAction.REQUEST_STATUS => requestStatus(appArgs)
    }
  }
 ...... 
```
ä¹‹åå°±ç›´æ¥çœ‹ä¸€ä¸‹è¿™ä¸ªæ„é€ æäº¤å‚æ•°çš„ç±»ï¼Œè¿™é‡Œç®€å•çœ‹ä¸‹å…¶ä¸­æ¯”è¾ƒæ€»è¦çš„åˆå§‹åŒ–æ–¹æ³•ï¼š
```
  .....
  
  // Set parameters from command line arguments
  try {
   1. å°†æäº¤è„šæœ¬ä¸­çš„å‚æ•°åšä¸€ä¸ªè½¬æ¢ï¼Œå¦‚--class, --masterç­‰
    parse(args.asJava)
  } catch {
    case e: IllegalArgumentException =>
      SparkSubmit.printErrorAndExit(e.getMessage())
  }
  // Populate `sparkProperties` map from properties file
 2. åˆå¹¶--conf ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ä¸­å’Œspark-default.confä¸­çš„å‚æ•°
  mergeDefaultSparkProperties()
 3. åˆ æ‰ä¸æ˜¯sparkç³»ç»Ÿçš„é…ç½®å‚æ•°
  // Remove keys that don't start with "spark." from `sparkProperties`.
  ignoreNonSparkProperties()
  // Use `sparkProperties` map along with env vars to fill in any missing parameters
  4. æ·»åŠ ä¸Šé»˜è®¤çš„é…ç½®å‚æ•°
  loadEnvironmentArguments()
  5. éªŒè¯é…ç½®å‚æ•°çš„æœ‰æ•ˆæ€§
  validateArguments()
  .......
// Action should be SUBMIT unless otherwise specifiedaction = Option(action).getOrElse(SUBMIT)
  
  ......   

```
å…³äºæ€ä¹ˆå®ç°çš„æˆ‘å°±ä¸å…·ä½“åˆ†æäº†ï¼Œå¤§å®¶æœ‰å…´è¶£è‡ªå·±çœ‹ä¸€ä¸‹æºä»£ç ï¼ˆå› ä¸ºè¿™ä¸ªä¸å±äºsparkè¿è¡Œçš„ä¸»æµç¨‹ï¼Œæ²¡æœ‰å¿…è¦è¿‡å¤šè§£é‡Šï¼‰ï¼š

```
protected final void parse(List<String> args) {
    1.ä¸‹é¢è¿™äº›åªæ˜¯åšäº†ä¸€ä¸ªå­—ç¬¦ä¸²çš„åŒ¹é…å®ç°ï¼Œå¹¶å¯¹åŒ¹é…å®Œçš„å‚æ•°è¿›è¡Œå¤„ç†ï¼Œè¿™é‡Œä¼šå¤„ç†ä¸‰ç§ä¸åŒç§ç±»çš„å‚æ•°
    Pattern eqSeparatedOpt = Pattern.compile("(--[^=]+)=(.+)");

    int idx = 0;
    for (idx = 0; idx < args.size(); idx++) {
      String arg = args.get(idx);
      String value = null;

      Matcher m = eqSeparatedOpt.matcher(arg);
      if (m.matches()) {
        arg = m.group(1);
        value = m.group(2);
      }

      // Look for options with a value.
      String name = findCliOption(arg, opts);
      if (name != null) {
        if (value == null) {
          if (idx == args.size() - 1) {
            throw new IllegalArgumentException(
                String.format("Missing argument for option '%s'.", arg));
          }
          idx++;
          value = args.get(idx);
        }
     2. å¤„ç†èƒ½å¤ŸåŒ¹é…åˆ°çš„çš„å‚æ•°ï¼Œå¦‚â€œ --class org.apache.spark.examples.SparkPi \
  --master spark://207.184.161.138:6066 \â€
        if (!handle(name, value)) {
          break;
        }
        continue;
      }

      // Look for a switch.
      name = findCliOption(arg, switches);
      if (name != null) {
        if (!handle(name, null)) {
          break;
        }
        continue;
      }
    3. å¤„ç†ä¸çŸ¥é“çš„å‚æ•°ï¼Œä½†æ˜¯è¢«"--"æ ‡è®°ï¼Œæœ¬ä¾‹å­ä¸­ä¸å­˜åœ¨è¿™ç§å‚æ•°
      if (!handleUnknown(arg)) {
        break;
      }
    }

    if (idx < args.size()) {
      idx++;
    }
   4. å¤„ç†ä¸æ˜¯æ²¡æœ‰è¢«"--"æ ‡è®°çš„å‚æ•°ï¼Œå¦‚æœ€åçš„â€œ  /path/to/examples.jar   â€
    handleExtraArgs(args.subList(idx, args.size()));
  }
  
```
æ‰€ä»¥è¿™é‡Œæ•´ç†ä¸€ä¸‹æ ¹æ®ä¸Šé¢çš„å¤„ç†è¿™é‡Œå¾—åˆ°çš„æœ‰ç”¨çš„ä¿¡æ¯ï¼š
1. å½“SparkSubmitArgumentsåˆå§‹åŒ–çš„æ—¶å€™ï¼Œé»˜è®¤ä¼šæŒ‡å®šactionä¸ºSUBMITï¼Œ
2. handleæ–¹æ³•ä¼šæŒ‡å®šmasterå’ŒmainClasså‚æ•°
3. handleExtraArgsä¼šå°†  /path/to/examples.jar   æ·»åŠ åˆ°childArgsä¸­ï¼Œä¹Ÿå°±æ˜¯æŒ‡å®šäº†childArgsè¿™ä¸ªå‚æ•°çš„å€¼ã€‚

### action : submit
æ‰€ä»¥actionä¼šåŒ¹é…åˆ°SparkSubmitAction.SUBMITï¼Œä»è€Œæ‰§è¡Œsubmitæ–¹æ³•ï¼Œè€Œåœ¨submitä¸­ï¼Œä¸¤ä¸ªé‡è¦çš„æ–¹æ³•ï¼š
```
  .....
     1. ç”¨æ¥æ„é€ å‡†å¤‡æäº¤appçš„é…ç½®å‚æ•°
     val (childArgs, childClasspath, sysProps, childMainClass) = prepareSubmitEnvironment(args)
    
  ......   
     2. å¯åŠ¨æäº¤ç¨‹åºçš„å®¢æˆ·ç«¯ï¼Œå¹¶æäº¤ç¨‹åº
                runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)
  ......   
```
å…ˆçœ‹prepareSubmitEnvironmentæ„é€ çš„ä¸€ä¸ªæ¯”è¾ƒé‡è¦çš„å‚æ•°å°±æ˜¯è¿™ä¸ªchildMainClassï¼Œä¼šæœ‰ä¸åŒçš„clientç¨‹åº
```
         1. standalone cluster å’Œmesos clusterï¼š
         childMainClass = "org.apache.spark.deploy.rest.RestSubmissionClient"
         2. yarn cluster
         childMainClass = "org.apache.spark.deploy.yarn.Client"
```
è€Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯standaloneæ¨¡å¼ï¼ˆå…¶ä»–æ¨¡å¼åŸç†éƒ½æ˜¯ä¸€æ ·çš„ï¼‰ï¼Œå®ƒè¿™é‡ŒæŒ‡å®šåé¢çš„runmainæ–¹æ³•è¦å¯åŠ¨çš„ç±»ï¼Œç„¶åçœ‹è¿™ä¸ªrunmainæ–¹æ³•ï¼š

```
     private def runMain(
      childArgs: Seq[String],
      childClasspath: Seq[String],
      sysProps: Map[String, String],
      childMainClass: String,
      verbose: Boolean): Unit = {
    // scalastyle:off println
    1. é¦–å…ˆæ‰“å°ä¸€äº›å‚æ•°çš„åŸºæœ¬ä¿¡æ¯
    if (verbose) {
      printStream.println(s"Main class:\n$childMainClass")
      printStream.println(s"Arguments:\n${childArgs.mkString("\n")}")
      printStream.println(s"System properties:\n${sysProps.mkString("\n")}")
      printStream.println(s"Classpath elements:\n${childClasspath.mkString("\n")}")
      printStream.println("\n")
    }
    // scalastyle:on println
   2. è¿™é‡Œä¼šæŒ‡å®šåŠ è½½ç±»å¯¹è±¡çš„classloaderï¼Œå¦‚æœå¼€å¯spark.driver.userClassPathFirstè¿™ä¸ªå¼€å…³çš„è¯ï¼Œloaderä¼šå…ˆä½¿ç”¨useræŒ‡å®šçš„jaråŒ…ï¼Œç„¶ååœ¨ä½¿ç”¨ç³»ç»Ÿé»˜è®¤çš„jarï¼Œè¿™é‡Œå®ƒçš„å®ç°å°±æ˜¯çŠ¹åœ¨è¿™ä¸ªChildFirstURLClassLoaderä¸­ç”Ÿå‘½äº†ä¸€ä¸ªParentClassLoaderï¼Œåœ¨ä½¿ç”¨çš„æ—¶å€™ï¼Œå…ˆä½¿ç”¨ChildFirstURLClassLoaderçš„jarï¼ˆuserï¼‰ï¼Œç„¶ååœ¨ä½¿ç”¨ParentClassLoaderçš„jarï¼ˆsysï¼‰ã€‚
    val loader =
      if (sysProps.getOrElse("spark.driver.userClassPathFirst", "false").toBoolean) {
        new ChildFirstURLClassLoader(new Array[URL](0),
          Thread.currentThread.getContextClassLoader)
      } else {
        new MutableURLClassLoader(new Array[URL](0),
          Thread.currentThread.getContextClassLoader)
      }
    Thread.currentThread.setContextClassLoader(loader)
    
    3. å°†jaråŒ…æ·»åŠ åˆ°loaderçš„classpathä¸‹
    for (jar <- childClasspath) {
      addJarToClasspath(jar, loader)
    }
   4. seté…ç½®ä¿¡æ¯
    for ((key, value) <- sysProps) {
      System.setProperty(key, value)
    }
   5. åé¢è¿™ä¸€å¤§æ®µä»£ç å°±æ˜¯é€šè¿‡åå°„çš„æ–¹å¼æ‰¾åˆ°mainClassï¼Œç„¶ååœ¨è·å¾—å…¶mainæ–¹æ³•ã€‚
    var mainClass: Class[_] = null

    try {
      mainClass = Utils.classForName(childMainClass)
    } catch {
      case e: ClassNotFoundException =>
        e.printStackTrace(printStream)
        if (childMainClass.contains("thriftserver")) {
          // scalastyle:off println
          printStream.println(s"Failed to load main class $childMainClass.")
          printStream.println("You need to build Spark with -Phive and -Phive-thriftserver.")
          // scalastyle:on println
        }
        System.exit(CLASS_NOT_FOUND_EXIT_STATUS)
      case e: NoClassDefFoundError =>
        e.printStackTrace(printStream)
        if (e.getMessage.contains("org/apache/hadoop/hive")) {
          // scalastyle:off println
          printStream.println(s"Failed to load hive class.")
          printStream.println("You need to build Spark with -Phive and -Phive-thriftserver.")
          // scalastyle:on println
        }
        System.exit(CLASS_NOT_FOUND_EXIT_STATUS)
    }

    // SPARK-4170
    if (classOf[scala.App].isAssignableFrom(mainClass)) {
      printWarning("Subclasses of scala.App may not work correctly. Use a main() method instead.")
    }

    val mainMethod = mainClass.getMethod("main", new Array[String](0).getClass)
    if (!Modifier.isStatic(mainMethod.getModifiers)) {
      throw new IllegalStateException("The main method in the given main class must be static")
    }

    def findCause(t: Throwable): Throwable = t match {
      case e: UndeclaredThrowableException =>
        if (e.getCause() != null) findCause(e.getCause()) else e
      case e: InvocationTargetException =>
        if (e.getCause() != null) findCause(e.getCause()) else e
      case e: Throwable =>
        e
    }
   6. å°±æ˜¯ç›´æ¥è°ƒç”¨è¿™ä¸ªmainæ–¹æ³•
    try {
      mainMethod.invoke(null, childArgs.toArray)
    } catch {
      case t: Throwable =>
        findCause(t) match {
          case SparkUserAppException(exitCode) =>
            System.exit(exitCode)

          case t: Throwable =>
            throw t
        }
    }
  }

```
ä¸‹é¢å°±ç›´æ¥çœ‹è¿™ä¸ªmainæ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯RestSubmissionClientçš„mainæ–¹æ³•ï¼š

```
     def main(args: Array[String]): Unit = {
    if (args.size < 2) {
      sys.error("Usage: RestSubmissionClient [app resource] [main class] [app args*]")
      sys.exit(1)
    }
    val appResource = args(0)
    val mainClass = args(1)
    val appArgs = args.slice(2, args.size)
    val conf = new SparkConf
    val env = filterSystemEnvironment(sys.env)
    run(appResource, mainClass, appArgs, conf, env)
  }
```
æ¥ç€ç›´æ¥çœ‹è¿™ä¸ªrunæ–¹æ³•ï¼š
```
   def run(
      appResource: String,
      mainClass: String,
      appArgs: Array[String],
      conf: SparkConf,
      env: Map[String, String] = Map()): SubmitRestProtocolResponse = {
    val master = conf.getOption("spark.master").getOrElse {
      throw new IllegalArgumentException("'spark.master' must be set.")
    }
    val sparkProperties = conf.getAll.toMap
    1. å¯åŠ¨æäº¤appçš„clientç¨‹åº
    val client = new RestSubmissionClient(master)
    2. æ„å»ºæäº¤çš„request
    val submitRequest = client.constructSubmitRequest(
      appResource, mainClass, appArgs, sparkProperties, env)
    3. æ­£å¼æäº¤è¿™ä¸ªrequest
    client.createSubmission(submitRequest)
  }
     
```
ä¸‹é¢ç›´æ¥çœ‹è¿™ä¸ªcreateSubmissionæ–¹æ³•ï¼š

```
   def createSubmission(request: CreateSubmissionRequest): SubmitRestProtocolResponse = {
    logInfo(s"Submitting a request to launch an application in $master.")
    var handled: Boolean = false
    var response: SubmitRestProtocolResponse = null
    for (m <- masters if !handled) {
      1. éªŒè¯é€šè¿‡spark-submitè„šæœ¬æäº¤çš„masterçš„æœ‰æ•ˆæ€§
      validateMaster(m)
      2. è·å–æäº¤appçš„urlï¼Œå½¢å¦‚"http://207.184.161.138:6066/v1/submissions/create"
      val url = getSubmitUrl(m)
      try {
      3. æ­£å¼å‘serverå‘é€è¿™ä¸ªurlï¼Œè¿™ä¸ªæ–¹æ³•å°±æ˜¯æ‰§è¡Œçš„è¿æ¥ï¼Œå†™æ“ä½œäº†
        response = postJson(url, request.toJson)
     4. å¯¹å“åº”ä¿¡æ¯çš„å¤„ç†
        response match {
          case s: CreateSubmissionResponse =>
            if (s.success) {
              reportSubmissionStatus(s)
              handleRestResponse(s)
              handled = true
            }
          case unexpected =>
            handleUnexpectedRestResponse(unexpected)
        }
      } catch {
        case e: SubmitRestConnectionException =>
          if (handleConnectionException(m)) {
            throw new SubmitRestConnectionException("Unable to connect to server", e)
          }
      }
    }
    response
  }
```
ç„¶åç›´æ¥çœ‹æˆ‘ä»¬çš„serverï¼Œè¿™é‡Œä½ ä¼šæœ‰ä¸€ä¸ªç–‘é—®ï¼Œè¿™æ˜¯å“ªé‡Œæ¥çš„serverï¼Ÿ
å…¶å®è¿™ä¸ªserveræˆ‘ä»¬æ—©å·²ç»åˆ›å»ºå¥½äº†ï¼Œå°±æ˜¯åœ¨new  Masterçš„æ—¶å€™åˆ›å»ºçš„ï¼š

```
  
    if (restServerEnabled) {
      val port = conf.getInt("spark.master.rest.port", 6066)
      restServer = Some(new StandaloneRestServer(address.host, port, conf, self, masterUrl))
    }
    restServerBoundPort = restServer.map(_.start())   
         
```
æ‰€ä»¥åœ¨Masterå®ä¾‹åŒ–çš„æ—¶å€™ï¼Œå·²ç»åˆ›å»ºå¹¶å¯åŠ¨äº†è¿™ä¸ªserverã€‚
å…³äºè¿™ä¸ªserveræ˜¯æ€ä¹ˆæ¥æ”¶æ¶ˆæ¯çš„ï¼Œè¿™ä¸ªåšè¿‡java webå¼€å‘çš„äººåº”è¯¥å¾ˆç†Ÿæ‚‰äº†ï¼Œæˆ‘è¿™é‡Œå°±ä¸åˆ†æäº†ï¼Œå¦‚æœç‰¹æƒ³çŸ¥é“æ˜¯æ€ä¹ˆå®ç°çš„ï¼Œå¤§å®¶æƒ³çœ‹ä¸€ä¸‹java webä¸­serveletè¿™ä¸ªæ¨¡å—çš„å†…å®¹ï¼Œç„¶åå†çœ‹è¿™é‡Œå°±ä¼šæ˜ç™½äº†

è¿™é‡Œç®€å•è¯´ä¸€ä¸‹ï¼š
åœ¨æœ¬ä¾‹ä¸­serverä½¿ç”¨jettyæ¥æ˜¯æƒ³çš„ï¼Œåœ¨serverå¯åŠ¨çš„è¿‡ç¨‹ä¸­ï¼Œä¼šç»‘å®šä¸€ä¸ªServletContextHandlerï¼Œç„¶åä¼šå°†serveletæ·»åŠ åˆ°ServletContextHandlerï¼Œæ‰€ä»¥æ•°æ®ä¼šé€šè¿‡serveletæ¥è¿›è¡Œå¤„ç†ï¼š
é¦–å…ˆStandaloneRestServeræ˜¯ç»§æ‰¿è‡ªRestSubmissionServerè¿™ä¸ªç±»çš„ï¼Œå…ˆçœ‹ä¸‹é‡Œé¢æœ‰ä¸€æ®µå…³é”®ä»£ç ï¼š

```
  
     protected lazy val contextToServlet = Map[String, RestServlet](
    s"$baseContext/create/*" -> submitRequestServlet,
    s"$baseContext/kill/*" -> killRequestServlet,
    s"$baseContext/status/*" -> statusRequestServlet,
    "/*" -> new ErrorServlet // default handler
  )
         
```
æœ‰è¿™é‡Œå¯ä»¥çœ‹å‡ºï¼Œæˆ‘ä»¬çš„å®ä¾‹ä¼šæœ€ç»ˆåŒ¹é…åˆ°submitRequestServletï¼Œæœ€ç»ˆå®ç°æ˜¯StandaloneSubmitRequestServletè¿™ä¸ªç±»ã€‚ç„¶åæ ¹æ®urlï¼Œä»–æœ€ç»ˆä¼šè°ƒç”¨handleSubmitè¿™ä¸ªæ–¹æ³•æ¥å¤„ç†æäº¤çš„request

```
  protected override def handleSubmit(
      requestMessageJson: String,
      requestMessage: SubmitRestProtocolMessage,
      responseServlet: HttpServletResponse): SubmitRestProtocolResponse = {
    requestMessage match {
      1. ä¸€èˆ¬éƒ½ä¼šåŒ¹é…åˆ°submitRequest
      case submitRequest: CreateSubmissionRequest =>
      2. æ„é€ driverå¯åŠ¨çš„å‚æ•°ä¿¡æ¯
        val driverDescription = buildDriverDescription(submitRequest)
      3. è¿™é‡Œæ˜¯æœ€æœ€å…³é”®çš„ä¸€æ­¥å°±æ˜¯å®ƒå‘masterå‘é€äº†RequestSubmitDriverçš„æ¶ˆæ¯ã€‚è‡³æ­¤æ•´ä¸ªappæäº¤çš„è¿‡ç¨‹å°±å®Œäº†
        val response = masterEndpoint.askWithRetry[DeployMessages.SubmitDriverResponse](
          DeployMessages.RequestSubmitDriver(driverDescription))
       4. åé¢å°±æ˜¯ä¸€äº›å“åº”çš„ä¿¡æ¯äº†
        val submitResponse = new CreateSubmissionResponse
        submitResponse.serverSparkVersion = sparkVersion
        submitResponse.message = response.message
        submitResponse.success = response.success
        submitResponse.submissionId = response.driverId.orNull
        val unknownFields = findUnknownFields(requestMessageJson, requestMessage)
        if (unknownFields.nonEmpty) {
          // If there are fields that the server does not know about, warn the client
          submitResponse.unknownFields = unknownFields
        }
        submitResponse
      case unexpected =>
        responseServlet.setStatus(HttpServletResponse.SC_BAD_REQUEST)
        handleError(s"Received message of unexpected type ${unexpected.messageType}.")
    }
  }
           
```
okï¼Œè‡³æ­¤æ•´ä¸ªæäº¤appçš„è¿‡ç¨‹åˆèµ°é€šäº†ï¼Œè¿™é‡Œåœ¨ç»“æŸå‰è¿˜å¾—çœ‹ä¸€ä¸‹è¿™ä¸ªbuildDriverDescriptionä¸­ä¸€æ®µæ¯”è¾ƒé‡è¦çš„ä»£ç ï¼š

```
 val command = new Command(
      "org.apache.spark.deploy.worker.DriverWrapper",
      Seq("{{WORKER_URL}}", "{{USER_JAR}}", mainClass) ++ appArgs, // args to the DriverWrapper
                 
```
è¿™é‡Œå…ˆçœ‹è¿™ä¸ªorg.apache.spark.deploy.worker.DriverWrapperè¿™ä¸ªç±»ï¼Œå…¶å®åœ¨ä¹‹ådriverçš„å¯åŠ¨å°±æ˜¯é€šè¿‡è¿™ä¸ªç±»æ¥å®ç°çš„ï¼Œå› ä¸ºç»å¸¸æœ‰äººä¼šé—®åˆ°åˆ°åº•driveræ€ä¹ˆå¯åŠ¨çš„ï¼Œç°åœ¨ä½ åº”è¯¥å¾ˆæ˜ç™½äº†å§ï¼ å¥½äº†ï¼Œå…ˆåˆ°è¿™é‡Œï¼Œåé¢æˆ‘ä»¬å°±è¯¥æ­£å¼å¼€å§‹ç¨‹åºçš„ä¸»æµç¨‹åˆ†æäº†ã€‚


è¡¥å……è¯´æ˜ï¼šå…³äºlegacyæ–¹å¼ï¼š
å½“ä½ ä½¿ç”¨ä¸‹é¢è¿™ä¸ªshellè„šæœ¬æäº¤ç¨‹åºæ—¶ï¼Œæ³¨æ„ä¸‹é¢çš„ç«¯å£ä¸º7077

```
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://207.184.161.138:7077 \
  /path/to/examples.jar   
```
ç¨‹åºè¿è¡Œä¼šå‘ç”Ÿå¼‚å¸¸çš„ï¼Œå› ä¸ºå¹¶æ²¡æœ‰ç«¯å£ä¸º7077çš„restServerã€‚è¿™ä¸ªæ—¶å€™ç¨‹åºè¿è¡Œå°±ä¼šå‘ç”Ÿä¸€ä¸‹çš„warningæç¤ºï¼š

```
Warning: Master endpoint spark://207.184.161.138:7077 was not a REST
server. Falling back to legacy submission gateway instead.

```
ä¸‹é¢æˆ‘æŠŠå‘ç”Ÿè¿™ä¸ªå¼‚å¸¸çš„ä½ç½®åˆ—å‡ºæ¥ï¼Œå°±åœ¨SparkSubmitç±»ä¸‹é¢çš„submitæ–¹æ³•ä¸­ï¼Œ
æˆ‘è¿™é‡Œæˆªå–é‡è¦ä»£ç ç‰‡æ®µï¼š



```
// In standalone cluster mode, there are two submission gateways:
     //   (1) The traditional Akka gateway using o.a.s.deploy.Client as a wrapper
     //   (2) The new REST-based gateway introduced in Spark 1.3
     // The latter is the default behavior as of Spark 1.3, but Spark submit will fail over
     // to use the legacy gateway if the master endpoint turns out to be not a REST server.
    1.å…ˆçœ‹ä»–æ³¨é‡Šè¿™é‡Œæ˜¯æœ‰è¯´æ˜çš„å…³äºappçš„æäº¤æœ‰ä¸¤ç§æ–¹å¼ï¼š
       1.1 é€šè¿‡Clientè¿™ä¸ªç±»æ¥æäº¤
       1.2 é€šè¿‡restçš„æ–¹å¼è¿›è¡Œæäº¤ï¼Œä¸Šé¢åšå®¢ä¸­æœ‰åˆ†æçš„å¾ˆæ¸…æ¥šï¼Œå°±æ˜¯é€šè¿‡RestSubmissionClientè¿™ä¸ªç±»æ¥å®ç°
    if (args.isStandaloneCluster && args.useRest) {
      try {
        // scalastyle:off println
        printStream.println("Running Spark using the REST application submission protocol.")
        // scalastyle:on println
        2. è¿™é‡Œå°±æ˜¯é€šè¿‡restæ–¹å¼æ¥æäº¤appçš„æ–¹æ³•
        doRunMain()
      } catch {
        // Fail over to use the legacy submission gateway
        case e: SubmitRestConnectionException =>
          3. çœ‹è¿™é‡Œè¿™ä¸ªwarningä¿¡æ¯æ­£å¥½å’Œä¸Šé¢çš„æ‰“å°ä¿¡æ¯æ˜¯ä¸€è‡´çš„
          printWarning(s"Master endpoint ${args.master} was not a REST server. " +
            "Falling back to legacy submission gateway instead.")
          args.useRest = false
          4.è¿™æ ·çš„è¯ï¼Œä»–å°±ä¼šå¯åŠ¨è¿™ä¸ªå‚æ•°ä¸­çš„ç±»
          submit(args)
      }
    // In all other modes, just run the main class as prepared
    } else {
      doRunMain()
    }
 
```
è€Œè¿™ä¸ªargså‚æ•°ä¸­çš„clientç±»ï¼Œæ˜¯åœ¨ä½¿ç”¨prepareSubmitEnvironmentæ–¹æ³•çš„æ—¶å€™æ„å»ºçš„ï¼Œä¸‹é¢çœ‹é‡è¦ä»£ç ç‰‡æ®µï¼š



```
    if (args.isStandaloneCluster) {
      if (args.useRest) {
        childMainClass = "org.apache.spark.deploy.rest.RestSubmissionClient"
        childArgs += (args.primaryResource, args.mainClass)
      } else {
        // In legacy standalone cluster mode, use Client as a wrapper around the user class
         1. érestæ–¹å¼ï¼Œclientä½¿ç”¨ä¸‹é¢è¿™ä¸ªç±»
        childMainClass = "org.apache.spark.deploy.Client"
        if (args.supervise) { childArgs += "--supervise" }
        Option(args.driverMemory).foreach { m => childArgs += ("--memory", m) }
        Option(args.driverCores).foreach { c => childArgs += ("--cores", c) }
        childArgs += "launch"
        childArgs += (args.master, args.primaryResource, args.mainClass)
      }
      if (args.childArgs != null) {
        childArgs ++= args.childArgs
      }
    } 
```
æ‰€ä»¥åœ¨ä¹‹ådoRunMainæ–¹æ³•ä¸­è°ƒç”¨é‚£ä¸ªç±»å°±æ˜¯org.apache.spark.deploy.Clientï¼š
ä¸‹é¢ç®€å•çœ‹ä¸€ä¸‹å®ƒçš„mainæ–¹æ³•ï¼Œä¸‹é¢æˆ‘æŒ‡å‡ºé‡è¦éƒ¨åˆ†ï¼š

```
    /**
 * Executable utility for starting and terminating drivers inside of a standalone cluster.
 */
object Client {
  def main(args: Array[String]) {
    // scalastyle:off println
    if (!sys.props.contains("SPARK_SUBMIT")) {
      println("WARNING: This client is deprecated and will be removed in a future version of Spark")
      println("Use ./bin/spark-submit with \"--master spark://host:port\"")
    }
    // scalastyle:on println

    val conf = new SparkConf()
    val driverArgs = new ClientArguments(args)

    if (!driverArgs.logLevel.isGreaterOrEqual(Level.WARN)) {
      conf.set("spark.akka.logLifecycleEvents", "true")
    }
    conf.set("spark.rpc.askTimeout", "10")
    conf.set("akka.loglevel", driverArgs.logLevel.toString.replace("WARN", "WARNING"))
    Logger.getRootLogger.setLevel(driverArgs.logLevel)
    1. å»ºç«‹ä¸€ä¸ªrpcEnv
    val rpcEnv =
      RpcEnv.create("driverClient", Utils.localHostName(), 0, conf, new SecurityManager(conf))
     2. è·å¾—masterçš„endpoint
    val masterEndpoints = driverArgs.masters.map(RpcAddress.fromSparkURL).
      map(rpcEnv.setupEndpointRef(Master.SYSTEM_NAME, _, Master.ENDPOINT_NAME))
    3. ç„¶åè¿™ä¸ªå®¢æˆ·ç«¯ï¼Œä¹Ÿæ–°å»ºä¸€ä¸ªClientEndpointçš„å¯¹è±¡ï¼Œç”¨æ¥å’Œmasteré€šä¿¡ï¼Œä»è€Œåœ¨æ„å»ºå®Œåº”ç”¨åï¼Œå¯ä»¥ç›´æ¥æäº¤
    rpcEnv.setupEndpoint("client", new ClientEndpoint(rpcEnv, driverArgs, masterEndpoints, conf))

    rpcEnv.awaitTermination()
  }
}

```

è®²åˆ°è¿™é‡Œï¼Œå¤§å®¶åº”è¯¥å¾ˆæ˜ç™½äº†å§ï¼å¦å¤–è¿™é‡Œå¤šäº›ä¸€ä¸‹@[æœ¨æœ¨å°šæœˆå…³](http://www.jianshu.com/users/5155fcec7fde)Â ï¼Œå› ä¸ºè¿™ä¸ªæäº¤åˆšå¼€å§‹è®²çš„æ—¶å€™ï¼Œæ²¡æœ‰è€ƒè™‘çš„å¤ªå¤šã€‚å†æ¬¡æ„Ÿè°¢ğŸ™ï¼ï¼ï¼









